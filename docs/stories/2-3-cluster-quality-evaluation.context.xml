<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>3</storyId>
    <title>Cluster Quality Evaluation</title>
    <status>drafted</status>
    <generatedAt>2025-11-09</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/2-3-cluster-quality-evaluation.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>data mining student</asA>
    <iWant>to evaluate cluster quality using standard metrics</iWant>
    <soThat>I can prove the clustering produces good semantic separation</soThat>
    <tasks>
      - Implement ClusteringMetrics class in src/evaluation/clustering_metrics.py (AC: #1-#7)
      - Create cluster quality evaluation script scripts/03_evaluate_clustering.py (AC: #8-#10)
      - Implement Silhouette Score calculation (AC: #1)
      - Implement Davies-Bouldin Index calculation (AC: #2)
      - Implement intra-cluster distance calculation (AC: #3)
      - Implement inter-cluster distance calculation (AC: #4)
      - Implement cluster purity calculation (AC: #5)
      - Implement confusion matrix generation (AC: #6)
      - Implement cluster balance validation (AC: #7)
      - Test cluster quality evaluation (AC: #1-#10)
      - Update project documentation (AC: all)
    </tasks>
  </story>

  <acceptanceCriteria>
    AC-1: Silhouette Score Calculation
    - Compute Silhouette Score for all 120K documents
    - Use sklearn.metrics.silhouette_score
    - Target: Score >0.3 (good cluster separation)
    - Save to data/processed/cluster_metadata.json

    AC-2: Davies-Bouldin Index Calculation
    - Compute Davies-Bouldin Index for all clusters
    - Use sklearn.metrics.davies_bouldin_score
    - Lower values indicate better separation
    - Save to data/processed/cluster_metadata.json

    AC-3: Intra-Cluster Distance (Compactness)
    - Calculate average distance from documents to cluster centroid
    - Compute per-cluster and overall compactness
    - Save to data/processed/cluster_metadata.json

    AC-4: Inter-Cluster Distance (Separation)
    - Calculate pairwise distances between cluster centroids
    - Compute min, max, mean inter-cluster distances
    - Save to data/processed/cluster_metadata.json

    AC-5: Cluster Purity Against Ground Truth
    - Compare with AG News ground truth categories
    - Compute per-cluster purity (dominant category percentage)
    - Target: Overall purity >0.7 (70%)
    - Save to data/processed/cluster_metadata.json

    AC-6: Confusion Matrix (Cluster vs Ground Truth)
    - Generate 4Ã—4 confusion matrix
    - Rows: AG News categories, Columns: Cluster IDs
    - Save as numpy array to data/processed/confusion_matrix.npy

    AC-7: Cluster Size Validation
    - Compute cluster sizes using np.bincount(labels)
    - Validate no cluster <10% or >50% of documents
    - Save to data/processed/cluster_metadata.json

    AC-8: Cluster Quality Report Export
    - Save all metrics to data/processed/cluster_quality.json
    - JSON formatted with indent=2
    - Append to existing cluster_metadata.json

    AC-9: Logging and Observability
    - Emoji-prefixed logs for visual clarity
    - Log all major operations with clear descriptions
    - Display summary at completion

    AC-10: Error Handling
    - Clear error if cluster assignments missing
    - Clear error if embeddings missing
    - Validation errors for shape/label mismatches
    - Automatic directory creation
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-2.md</path>
        <title>Epic Technical Specification: K-Means Clustering Implementation</title>
        <section>Detailed Design â†’ Services and Modules â†’ ClusteringMetrics</section>
        <snippet>ClusteringMetrics (src/evaluation/clustering_metrics.py): Calculates Silhouette Score (target: >0.3), Davies-Bouldin Index (lower is better), computes cluster size distribution, validates cluster balance (no cluster <10% or >50% of data).</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.md</path>
        <title>Epic Technical Specification: K-Means Clustering Implementation</title>
        <section>Data Models and Contracts â†’ Output Data Models</section>
        <snippet>Cluster quality metrics include silhouette_score (float, target >0.3), davies_bouldin_index (float, lower is better), intra_cluster_distance (dict with per-cluster and overall values), inter_cluster_distance (dict with min, max, mean), cluster_purity (dict with per-cluster and overall purity), cluster_sizes (list of int).</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.md</path>
        <title>Epic Technical Specification: K-Means Clustering Implementation</title>
        <section>Acceptance Criteria â†’ AC-2: Cluster Quality Metrics</section>
        <snippet>Silhouette Score calculated and >0.3 (good cluster separation), Davies-Bouldin Index calculated (lower is better), cluster sizes computed and validated (no cluster <10% or >50% of data), all metrics exported to cluster_metadata.json.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Cluster Quality Evaluation</section>
        <snippet>The system evaluates clustering quality using standard metrics (Silhouette Score, Davies-Bouldin Index) to validate semantic separation. Target Silhouette Score >0.3 indicates good cluster separation. Cluster purity measures alignment with AG News ground truth categories.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/context_aware_multi_agent_system/models/clustering.py</path>
        <kind>model</kind>
        <symbol>KMeansClustering</symbol>
        <lines>1-100</lines>
        <reason>K-Means clustering implementation that generates cluster labels and centroids needed for quality evaluation</reason>
      </artifact>
      <artifact>
        <path>src/context_aware_multi_agent_system/utils/reproducibility.py</path>
        <kind>utility</kind>
        <symbol>set_seed</symbol>
        <lines>1-50</lines>
        <reason>Reproducibility helper function to call at script start (set_seed(42))</reason>
      </artifact>
      <artifact>
        <path>src/context_aware_multi_agent_system/config.py</path>
        <kind>configuration</kind>
        <symbol>Config, Paths</symbol>
        <lines>1-100</lines>
        <reason>Configuration management classes for loading config.yaml and resolving paths</reason>
      </artifact>
      <artifact>
        <path>scripts/02_train_clustering.py</path>
        <kind>script</kind>
        <symbol>main</symbol>
        <lines>1-200</lines>
        <reason>Reference for script structure, logging patterns, and clustering workflow</reason>
      </artifact>
      <artifact>
        <path>src/context_aware_multi_agent_system/data/load_dataset.py</path>
        <kind>data</kind>
        <symbol>load_ag_news</symbol>
        <lines>1-100</lines>
        <reason>Dataset loader for AG News ground truth labels needed for purity calculation</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="scikit-learn" version=">=1.7.2">For silhouette_score, davies_bouldin_score, euclidean_distances metrics</package>
        <package name="numpy" version=">=1.24.0">For array operations, bincount, linalg.norm</package>
        <package name="pandas" version=">=2.0.0">For data manipulation (if needed)</package>
        <package name="PyYAML" version=">=6.0">For config.yaml loading</package>
        <package name="datasets" version=">=2.14.0">For AG News ground truth labels</package>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    - Performance: Metric calculation <3 minutes for 120K documents (NFR-1 from PRD)
    - Reproducibility: No randomization involved, deterministic metrics
    - Logging: Use emoji-prefixed logging (ðŸ“Š, âœ…, âš ï¸, âŒ) from utils/logger.py
    - Error Handling: Validate input file existence and data schema before computation
    - Initialization Order: set_seed â†’ load config â†’ setup logger â†’ validate â†’ execute
    - Data Loading: Check file exists â†’ load â†’ validate â†’ process
    - File Naming: snake_case for modules (clustering_metrics.py), PascalCase for classes (ClusteringMetrics)
    - Configuration Access: No hardcoded values, all thresholds from config.yaml
  </constraints>

  <interfaces>
    <interface>
      <name>ClusteringMetrics API</name>
      <kind>class</kind>
      <signature>
        class ClusteringMetrics:
            def __init__(self, embeddings: np.ndarray, labels: np.ndarray, centroids: np.ndarray, ground_truth: np.ndarray)
            def calculate_silhouette_score(self) -> float
            def calculate_davies_bouldin_index(self) -> float
            def calculate_intra_cluster_distance(self) -> dict
            def calculate_inter_cluster_distance(self) -> dict
            def calculate_cluster_purity(self) -> dict
            def generate_confusion_matrix(self) -> np.ndarray
            def validate_cluster_balance(self) -> tuple[bool, dict]
            def evaluate_all(self) -> dict
      </signature>
      <path>src/context_aware_multi_agent_system/evaluation/clustering_metrics.py</path>
    </interface>
    <interface>
      <name>Input: Cluster Assignments</name>
      <kind>file</kind>
      <signature>CSV file with columns: document_id, cluster_id, category_label</signature>
      <path>data/processed/cluster_assignments.csv</path>
    </interface>
    <interface>
      <name>Input: Embeddings</name>
      <kind>file</kind>
      <signature>NumPy array (120000, 768) float32</signature>
      <path>data/embeddings/train_embeddings.npy</path>
    </interface>
    <interface>
      <name>Input: Centroids</name>
      <kind>file</kind>
      <signature>NumPy array (4, 768) float32</signature>
      <path>data/processed/centroids.npy</path>
    </interface>
    <interface>
      <name>Output: Cluster Quality Metrics</name>
      <kind>file</kind>
      <signature>JSON file with silhouette_score, davies_bouldin_index, intra/inter cluster distances, purity, sizes</signature>
      <path>data/processed/cluster_quality.json</path>
    </interface>
    <interface>
      <name>Output: Confusion Matrix</name>
      <kind>file</kind>
      <signature>NumPy array (4, 4) int64</signature>
      <path>data/processed/confusion_matrix.npy</path>
    </interface>
  </interfaces>

  <tests>
    <standards>
      - Use pytest framework for all tests
      - Create tests/epic2/test_clustering_metrics.py for unit tests
      - Map tests to acceptance criteria (AC-1, AC-2, etc.)
      - Use pytest.raises() for exception testing
      - Use pytest fixtures for test setup (temp directories, mock data)
      - Test both unit (small synthetic data) and integration (full dataset)
      - Expected test coverage: >90% for ClusteringMetrics class
    </standards>
    <locations>
      - tests/epic2/test_clustering_metrics.py (unit tests)
      - tests/epic2/test_clustering_integration.py (integration tests)
    </locations>
    <ideas>
      - AC-1: Test Silhouette Score calculation on small synthetic dataset (1000 samples), verify range [-1, 1]
      - AC-2: Test Davies-Bouldin Index >0, valid range
      - AC-3: Test intra-cluster distance calculation returns dict with per-cluster and overall values
      - AC-4: Test inter-cluster distance calculation returns min, max, mean values
      - AC-5: Test cluster purity calculation returns dict with per-cluster and overall purity in range [0, 1]
      - AC-6: Test confusion matrix shape (4, 4), sum equals total document count
      - AC-7: Test cluster balance validation detects imbalance (cluster <10% or >50%)
      - AC-8: Integration test verifying all outputs exist and have correct schema
      - AC-9: Integration test running full script on actual cluster results, verify targets met (Silhouette >0.3, Purity >0.7)
      - AC-10: Negative tests for missing files (cluster assignments, embeddings), shape mismatches
    </ideas>
  </tests>
</story-context>
