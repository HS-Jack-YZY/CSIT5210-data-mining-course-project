<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>5</epicId>
    <storyId>2</storyId>
    <title>Hierarchical Agglomerative Clustering with Dendrogram</title>
    <status>drafted</status>
    <generatedAt>2025-11-09</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/5-2-hierarchical-agglomerative-clustering.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a data mining student</asA>
    <iWant>to apply hierarchical clustering and visualize the dendrogram</iWant>
    <soThat>I can understand the hierarchical structure of news categories and compare with flat clustering approaches</soThat>
    <tasks>
      - Implement HierarchicalClustering class in src/models/hierarchical_clustering.py (AC: #1, #2, #4)
      - Create dendrogram visualization module src/visualization/dendrogram_plot.py (AC: #3, #10)
      - Create hierarchical clustering script scripts/06_hierarchical_clustering.py (AC: #1-#10)
      - Implement linkage method comparison (AC: #2)
      - Implement cluster quality metrics calculation (AC: #4)
      - Generate dendrogram visualization (AC: #3)
      - Export cluster assignments (AC: #5)
      - Implement memory and performance monitoring (AC: #6)
      - Test hierarchical clustering (AC: #1-#10)
      - Update project documentation (AC: all)
    </tasks>
  </story>

  <acceptanceCriteria>
    AC-1: Hierarchical Clustering Execution - Runs agglomerative clustering on 120K embeddings with ward linkage, n_clusters=4, saves assignments to CSV
    AC-2: Linkage Method Comparison - Tests ward/complete/average/single methods, calculates metrics, selects best by Silhouette Score
    AC-3: Dendrogram Visualization Generation - Creates dendrogram showing hierarchical merge structure with scipy.cluster.hierarchy
    AC-4: Cluster Quality Evaluation - Calculates Silhouette Score, Davies-Bouldin Index, cluster purity, saves to JSON
    AC-5: Cluster Assignments Export - Saves assignments CSV with document_id, cluster_id, ground_truth_category, linkage_method
    AC-6: Memory and Performance Optimization - Monitors memory/runtime, implements sampling if memory exceeds 16GB
    AC-7: Logging and Observability - Emoji-prefixed logs, progress bars, timing information, final summary
    AC-8: Error Handling - Clear errors for missing files, memory issues, validation failures
    AC-9: Reproducibility - Deterministic results, fixed sampling random_state=42
    AC-10: Dendrogram Interpretation Guidance - Includes interpretation notes explaining how to read dendrogram
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-5.md</path>
        <title>Epic Technical Specification: Alternative Clustering Algorithms Exploration</title>
        <section>Story 5.2 - Hierarchical Clustering Implementation</section>
        <snippet>Apply agglomerative clustering with multiple linkage methods (ward, complete, average, single). Generate dendrogram visualization showing hierarchical structure. Compare linkage methods for best performance. Evaluate cluster quality at n_clusters=4 for fair comparison.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-5.md</path>
        <title>Epic Technical Specification: Alternative Clustering Algorithms Exploration</title>
        <section>Hierarchical Clustering API</section>
        <snippet>HierarchicalClustering class with fit_predict returning labels and dendrogram data, compare_linkage_methods testing ward/complete/average/single methods, and calculate_metrics for cluster quality evaluation.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-5.md</path>
        <title>Epic Technical Specification: Alternative Clustering Algorithms Exploration</title>
        <section>Performance - Hierarchical Clustering Targets</section>
        <snippet>Single AgglomerativeClustering fit: less than 20 minutes (ward linkage). Linkage method comparison (4 methods): less than 2 hours total. Dendrogram generation may require sampling (10K samples) if full dataset too large. Memory usage: less than 8GB RAM.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-5.md</path>
        <title>Epic Technical Specification: Alternative Clustering Algorithms Exploration</title>
        <section>Data Models - Hierarchical Clustering Assignments</section>
        <snippet>CSV schema: document_id (int), cluster_id (int 0-3), ground_truth_category (str), linkage_method (str ward/complete/average/single)</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/context_aware_multi_agent_system/evaluation/clustering_metrics.py</path>
        <kind>evaluation_module</kind>
        <symbol>ClusteringMetrics</symbol>
        <lines>15-300</lines>
        <reason>Reusable metrics calculation for Silhouette Score and Davies-Bouldin Index - same class used for K-Means evaluation</reason>
      </artifact>
      <artifact>
        <path>src/context_aware_multi_agent_system/evaluation/cluster_analysis.py</path>
        <kind>evaluation_module</kind>
        <symbol>ClusterAnalyzer</symbol>
        <lines>20-400</lines>
        <reason>Reusable cluster purity calculation and category mapping - supports hierarchical clustering via map_clusters_to_categories() and calculate_cluster_purity() methods</reason>
      </artifact>
      <artifact>
        <path>src/context_aware_multi_agent_system/visualization/cluster_plots.py</path>
        <kind>visualization_module</kind>
        <symbol>PCAVisualizer</symbol>
        <lines>23-200</lines>
        <reason>PCA visualization infrastructure - can be reused for dendrogram companion plots if needed</reason>
      </artifact>
      <artifact>
        <path>src/context_aware_multi_agent_system/config.py</path>
        <kind>config_module</kind>
        <symbol>Config, Paths</symbol>
        <lines>1-150</lines>
        <reason>Configuration management - use for accessing clustering parameters and file paths</reason>
      </artifact>
      <artifact>
        <path>data/embeddings/train_embeddings.npy</path>
        <kind>input_data</kind>
        <symbol>N/A</symbol>
        <lines>N/A</lines>
        <reason>Input embeddings (120K √ó 768) generated in Story 2.1 - required for hierarchical clustering</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="scikit-learn" version=">=1.7.2" usage="AgglomerativeClustering, silhouette_score, davies_bouldin_score"/>
        <package name="scipy" version=">=1.9.0" usage="scipy.cluster.hierarchy for dendrogram generation"/>
        <package name="numpy" version=">=1.24.0" usage="Array operations"/>
        <package name="pandas" version=">=2.0.0" usage="DataFrames for linkage comparison results"/>
        <package name="matplotlib" version=">=3.7.0" usage="Dendrogram plotting"/>
        <package name="psutil" version=">=5.9.0" usage="Memory monitoring"/>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    - Performance: Single hierarchical fit must complete in less than 20 minutes (ward linkage)
    - Performance: Linkage method comparison (4 methods) must complete in less than 2 hours total
    - Memory: Target less than 8GB RAM usage; implement sampling strategy if exceeded (threshold: 16GB)
    - Reproducibility: Hierarchical clustering is deterministic (no random initialization unlike K-Means)
    - Reproducibility: If sampling used, must use random_state=42 for reproducibility
    - Data Format: Use same embeddings as K-Means (data/embeddings/train_embeddings.npy) for fair comparison
    - Data Format: Cluster assignments must use same CSV schema as K-Means with ground_truth_category column
    - Data Format: Metrics must follow same JSON schema pattern for cross-algorithm comparison
    - Logging: Use emoji-prefixed logging (üìä INFO, ‚úÖ SUCCESS, ‚ö†Ô∏è WARNING, ‚ùå ERROR) from utils/logger.py
    - Error Handling: Clear errors with actionable troubleshooting steps (e.g., "Run script XX.py first")
    - Architecture: Follow Cookiecutter Data Science structure (src/models/, src/visualization/, scripts/, results/)
    - Architecture: Initialization order must be: set_seed ‚Üí load config ‚Üí setup logger ‚Üí validate ‚Üí execute
    - Architecture: File naming: snake_case for modules, PascalCase for classes
    - Architecture: No hardcoded values - all parameters from config.yaml
    - Testing: Type hints required for all methods (Google-style docstrings)
    - Testing: Map tests to acceptance criteria (AC-1, AC-2, etc.)
  </constraints>
  <interfaces>
    <interface>
      <name>HierarchicalClustering.__init__</name>
      <kind>class_constructor</kind>
      <signature>def __init__(self, n_clusters: int = 4, linkage: str = 'ward')</signature>
      <path>src/context_aware_multi_agent_system/models/hierarchical_clustering.py</path>
    </interface>
    <interface>
      <name>HierarchicalClustering.fit_predict</name>
      <kind>method</kind>
      <signature>def fit_predict(self, embeddings: np.ndarray) -> Tuple[np.ndarray, dict]</signature>
      <path>src/context_aware_multi_agent_system/models/hierarchical_clustering.py</path>
      <returns>labels: Cluster labels (n_samples,) int32 [0, 3], dendrogram_data: Dict containing linkage matrix</returns>
    </interface>
    <interface>
      <name>HierarchicalClustering.compare_linkage_methods</name>
      <kind>method</kind>
      <signature>def compare_linkage_methods(self, embeddings: np.ndarray, methods: List[str]) -> pd.DataFrame</signature>
      <path>src/context_aware_multi_agent_system/models/hierarchical_clustering.py</path>
      <returns>DataFrame with columns: linkage_method, silhouette_score, davies_bouldin, cluster_purity, runtime_seconds</returns>
    </interface>
    <interface>
      <name>HierarchicalClustering.calculate_metrics</name>
      <kind>method</kind>
      <signature>def calculate_metrics(self, labels: np.ndarray, embeddings: np.ndarray, ground_truth: np.ndarray) -> dict</signature>
      <path>src/context_aware_multi_agent_system/models/hierarchical_clustering.py</path>
      <returns>Dict with metrics: silhouette_score, davies_bouldin_index, cluster_purity, cluster_sizes</returns>
    </interface>
    <interface>
      <name>generate_dendrogram</name>
      <kind>function</kind>
      <signature>def generate_dendrogram(embeddings: np.ndarray, linkage_method: str = 'ward', output_path: Path, truncate_mode: str = 'lastp', p: int = 30) -> Path</signature>
      <path>src/visualization/dendrogram_plot.py</path>
      <returns>Path to saved dendrogram PNG file</returns>
    </interface>
    <interface>
      <name>ClusteringMetrics.calculate_silhouette_score</name>
      <kind>method</kind>
      <signature>def calculate_silhouette_score(self) -> float</signature>
      <path>src/context_aware_multi_agent_system/evaluation/clustering_metrics.py</path>
      <returns>Silhouette Score (higher = better, range [-1, 1])</returns>
    </interface>
    <interface>
      <name>ClusterAnalyzer.calculate_cluster_purity</name>
      <kind>method</kind>
      <signature>def calculate_cluster_purity(self) -> float</signature>
      <path>src/context_aware_multi_agent_system/evaluation/cluster_analysis.py</path>
      <returns>Cluster purity (% documents matching dominant category)</returns>
    </interface>
  </interfaces>
  <tests>
    <standards>
      Use pytest framework for all tests. Follow testing patterns from Story 2.2-2.5 (K-Means implementation).
      Unit tests on small synthetic data (100 samples, 10D). Integration tests on actual embeddings (120K √ó 768).
      Map tests to acceptance criteria (AC-1 through AC-10). Use pytest fixtures for test data.
      Test both normal execution and error conditions. All tests must have type hints and docstrings.
      Performance tests verify runtime targets (single fit less than 20 min, linkage comparison less than 2 hours).
      Reproducibility tests verify deterministic results across multiple runs.
    </standards>
    <locations>
      tests/epic5/test_hierarchical_clustering.py - Unit tests for HierarchicalClustering class
      tests/epic5/test_hierarchical_pipeline.py - Integration tests for full pipeline
      tests/epic5/test_dendrogram.py - Dendrogram generation tests
    </locations>
    <ideas>
      AC-1: Test hierarchical clustering execution - fit_predict on synthetic data returns correct shape (n_samples,) with labels in [0, 3]
      AC-2: Test linkage method comparison - compare_linkage_methods returns DataFrame with 4 rows (ward/complete/average/single)
      AC-3: Test dendrogram generation - generate_dendrogram creates PNG file at specified path with 300 DPI
      AC-4: Test cluster quality metrics - calculate_metrics returns dict with required keys (silhouette_score, davies_bouldin_index, cluster_purity, cluster_sizes)
      AC-5: Test assignments export - CSV file exists with correct schema (document_id, cluster_id, ground_truth_category, linkage_method) and 120K rows
      AC-6: Test memory monitoring - verify psutil tracks memory usage and logs warnings if exceeds 16GB threshold
      AC-7: Test logging - verify emoji-prefixed logs appear for major operations (üìä, ‚úÖ, ‚ö†Ô∏è, ‚ùå)
      AC-8: Test error handling - verify FileNotFoundError when embeddings missing, ValueError when wrong dimensions
      AC-9: Test reproducibility - run hierarchical clustering twice, verify identical results (deterministic algorithm)
      AC-10: Test dendrogram interpretation - verify dendrogram has title, axis labels, and n_clusters=4 boundary marker
      Integration: Test full pipeline on actual embeddings - verify all outputs created (assignments CSV, metrics JSON, dendrogram PNG, linkage comparison CSV)
      Performance: Benchmark runtime on 10K sample, extrapolate to 120K, verify within targets
      Negative: Test missing embeddings file raises clear error with actionable message
      Negative: Test wrong embedding dimensions (not 768D) raises ValueError
    </ideas>
  </tests>
</story-context>
