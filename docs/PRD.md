# Context-Aware Multi-Agent System for LLM Cost Optimization - Product Requirements Document

**Author:** Jack YUAN
**Date:** 2025-11-08
**Version:** 1.0

---

## Executive Summary

This project demonstrates how **clustering and classification algorithms** solve a critical real-world problem: reducing Large Language Model (LLM) API costs by 90%+ when processing long-context conversations. By applying K-Means clustering to partition documents into semantic groups and using cosine similarity classification to route queries to specialized agents, the system eliminates wasteful token consumption while maintaining response accuracy.

**The Core Innovation:** Transform an expensive monolithic LLM call (sending 200K+ tokens every time) into intelligent routing that sends only 20K-40K relevant tokensâ€”cutting costs by an order of magnitude.

**Academic Context:** Data Mining Course Project showcasing clustering and classification techniques applied to LLM cost optimization.

### What Makes This Special

**The Magic of Visualization:** When you see the cluster plot with 4 clearly separated semantic groups (World, Sports, Business, Sci/Tech), you immediately understand WHY this works. The visual separation proves that clustering captures meaningful semantic boundariesâ€”and those boundaries drive the cost savings.

**The "Wow" Moments:**
1. **Visual Proof**: PCA visualization showing 4 distinct clusters with minimal overlap
2. **Algorithmic Clarity**: Watching K-Means converge and cosine similarity classify queries with >80% accuracy
3. **Cost Impact**: Baseline $180 (100 queries Ã— $1.80) â†’ Optimized $18 (100 queries Ã— $0.18) = 90% reduction

**Why This Resonates:** It's not just a technical exerciseâ€”it demonstrates how fundamental data mining techniques (clustering + classification) solve expensive real-world problems in the age of AI.

---

## Project Classification

**Technical Type:** Python-based Multi-Agent System (Academic Prototype)
**Domain:** Machine Learning / Data Mining / LLM Optimization
**Complexity:** Medium (3-day implementation scope)

**Project Type Details:**
- **Category**: Academic proof-of-concept demonstrating clustering + classification
- **Primary Users**: Course instructors evaluating data mining technique mastery
- **Secondary Value**: Developers exploring LLM cost optimization strategies
- **Delivery Format**: Code + Experimental Report + Visual Demo

---

## Success Criteria

### Academic Success (Primary)

**For Course Evaluation:**
1. **Clear Demonstration of Clustering**
   - K-Means algorithm correctly partitions AG News into 4 semantic clusters
   - Silhouette Score >0.3 proves good cluster separation
   - PCA visualization clearly shows 4 distinct groups with minimal overlap
   - Can explain: initialization (k-means++), convergence, centroid calculation

2. **Clear Demonstration of Classification**
   - Cosine similarity classification achieves >80% accuracy routing queries to correct cluster
   - Can explain: embedding space, similarity metrics, nearest neighbor decision
   - Visual confusion matrix shows classification performance

3. **Reproducible Experimental Results**
   - All experiments documented with clear methodology
   - Code is clean, commented, and runnable
   - Results are verifiable by course instructor

4. **Real-World Problem Connection**
   - Demonstrates practical application to LLM cost optimization
   - Shows measurable cost reduction (>90% vs baseline)
   - Connects data mining theory to AI industry challenges

### Technical Success Metrics

**Must Achieve:**
- âœ… **Cost Reduction**: >90% reduction in LLM API calls compared to baseline
- âœ… **Classification Accuracy**: >80% correct cluster assignment for test queries
- âœ… **Cluster Quality**: Silhouette Score >0.3 (indicates good separation)
- âœ… **Response Time**: Query classification <1 second
- âœ… **Visual Clarity**: Cluster visualization clearly shows 4 separated groups

**Nice to Have:**
- ðŸŽ¯ Classification accuracy >85%
- ðŸŽ¯ Silhouette Score >0.4 (excellent separation)
- ðŸŽ¯ Cost reduction >95%

### Deliverable Quality Criteria

**Code Quality:**
- Clean Python implementation (PEP 8 compliant)
- Well-commented explaining clustering and classification steps
- Reproducible with clear instructions
- Minimal dependencies (numpy, pandas, scikit-learn, google-genai)

**Experimental Report Quality:**
- Clear methodology section explaining K-Means and cosine similarity
- Quantitative results with graphs and metrics
- Visual cluster plots (PCA 2D projection)
- Cost comparison charts (baseline vs optimized)
- Discussion connecting results to course concepts

**Demo Quality:**
- Live demonstration showing query routing
- Visual display of cluster assignments
- Cost calculation walkthrough

---

## Product Scope

### MVP - Minimum Viable Product (3-Day Timeline)

**Core Deliverables for Course Submission:**

**1. Data Preparation & Embedding Generation**
- Load AG News dataset (4 categories: World, Sports, Business, Sci/Tech)
- Generate embeddings for all documents using Google Gemini Embedding API
- Use Batch API for cost efficiency during bulk embedding
- Store embeddings for clustering phase

**2. K-Means Clustering Implementation**
- Apply K-Means clustering with K=4 to partition documents by semantic category
- Use k-means++ initialization for better convergence
- Calculate cluster centroids (will be used for classification)
- Evaluate cluster quality using Silhouette Score
- Generate PCA 2D visualization showing 4 separated clusters
- Document clustering parameters (K, random_state, max_iter)

**3. Specialized Agent Creation**
- Implement `SpecializedAgent` class holding context subset for one cluster
- Create 4 agents, each assigned documents from one cluster
- Each agent maintains only ~25% of total context (1/K reduction)
- Simple agent interface: `process_query(query)` â†’ response

**4. Cosine Similarity Classification & Routing**
- Implement `AgentRouter` class with classification logic
- For new queries: generate embedding â†’ compute cosine similarity with centroids â†’ route to nearest cluster's agent
- Track classification decisions for accuracy measurement
- Measure classification time (<1 second requirement)

**5. Baseline Comparison System**
- Implement naive baseline: single agent with full context (all documents)
- Baseline simulates current LLM usage pattern (send everything every time)
- Track: number of API calls, total tokens, estimated cost
- Compare baseline vs optimized system side-by-side

**6. Cost Metrics & Performance Measurement**
- **Metrics to Track:**
  - Number of LLM API calls (baseline vs optimized)
  - Total tokens consumed per query
  - Cost per query (using Claude Sonnet 4.5 pricing: $3/1M for <200K, $6/1M for >200K)
  - Classification accuracy (% correct cluster assignments)
  - Silhouette Score (cluster quality)
  - Response time per query
- **Target:** Demonstrate >90% cost reduction

**7. Experimental Report & Visualizations**
- **Methodology Section:** Explain K-Means and cosine similarity algorithms
- **Results Section:** Quantitative metrics tables and graphs
- **Visualizations:**
  - PCA cluster plot (4 colored groups)
  - Cost comparison bar chart (baseline vs optimized)
  - Classification confusion matrix
  - Silhouette analysis plot
- **Discussion:** Connect results to data mining course concepts
- **Conclusion:** Summarize findings and implications

**MVP Success Criteria:**
- âœ… All 7 components implemented and working
- âœ… Achieves >90% cost reduction vs baseline
- âœ… >80% classification accuracy
- âœ… Clear cluster visualization
- âœ… Complete experimental report with visualizations
- âœ… Runnable code with clear documentation

### Growth Features (Post-MVP / Future Enhancements)

**If Time Permits or for Future Exploration:**

**Enhanced Clustering:**
- Test different K values (K=3, K=5, K=6) and compare performance
- Implement Elbow method for automatic K selection
- Try alternative clustering algorithms (MiniBatchKMeans for scale, DBSCAN for density-based)
- Cross-validate cluster stability

**Advanced Classification:**
- Implement K-Nearest Neighbors (KNN) classifier as alternative to cosine similarity
- Train lightweight neural network classifier
- Ensemble classification (combine multiple methods)
- Confidence thresholding (route to multiple agents if uncertainty high)

**Multi-Dataset Validation:**
- Test on 20 Newsgroups dataset (20 categories)
- Test on DBpedia (more categories)
- Synthetic conversation datasets
- Compare performance across datasets

**Agent Enhancements:**
- Agent memory (maintain conversation state)
- Multi-agent collaboration (query spans multiple clusters)
- Fallback strategies (when classification confidence low)
- Agent specialization levels (sub-clustering within clusters)

**Production Readiness:**
- API endpoint for real-time query routing
- Caching layer for frequently accessed contexts
- Monitoring and logging infrastructure
- A/B testing framework

### Vision (Future Research Directions)

**Long-Term Possibilities Beyond Course Scope:**

**Dynamic Clustering:**
- Real-time cluster updates as new documents arrive
- Online learning for cluster adaptation
- Hierarchical clustering for multi-level routing
- Temporal clustering (clusters evolve over time)

**Advanced Routing Strategies:**
- Semantic routing with multiple LLMs (route to different models based on query type)
- Cost-accuracy trade-off optimization (choose agent based on budget constraints)
- Hybrid RAG + Clustering (combine retrieval with clustering)
- Query decomposition (split complex queries across agents)

**Enterprise Applications:**
- Customer service chatbot optimization (route by topic)
- Legal document analysis (cluster by case type)
- Medical record processing (cluster by specialty)
- Code documentation systems (cluster by module/language)

**Research Contributions:**
- Publish cost optimization methodology
- Open-source framework for LLM cost reduction
- Benchmark suite for clustering-based routing
- Novel clustering algorithms optimized for LLM contexts

**Note:** MVP is tightly scoped for 3-day timeline. Growth and Vision features are explicitly out of scope for course submission but represent natural extensions of the core concept.

---

---

## Python Multi-Agent System Specific Requirements

### System Architecture Requirements

**Core Components:**

1. **Embedding Service Layer**
   - Interface with Google Gemini Embedding API
   - Handle batch embedding for initial dataset processing
   - Handle real-time embedding for query classification
   - Error handling and retry logic for API calls
   - Embedding caching to avoid redundant API calls

2. **Clustering Engine**
   - K-Means implementation using scikit-learn
   - Configurable parameters (K, random_state, max_iter, init method)
   - Cluster quality evaluation (Silhouette Score, Davies-Bouldin Index)
   - Centroid extraction and storage
   - Visualization generation (PCA 2D projection)

3. **Agent Management System**
   - `SpecializedAgent` class definition
   - Agent initialization with cluster-specific context
   - Agent registry (map cluster_id â†’ agent instance)
   - Agent interface standardization

4. **Classification & Routing Module**
   - `AgentRouter` class with classification logic
   - Cosine similarity computation
   - Cluster assignment based on similarity scores
   - Routing decision tracking for accuracy measurement
   - Performance metrics collection

5. **Baseline Comparison Framework**
   - Single-agent baseline implementation
   - Token counting utilities
   - Cost calculation engine
   - Side-by-side comparison utilities

### Data Pipeline Requirements

**Input Data Handling:**
- Load AG News dataset from Hugging Face `datasets` library
- Support for CSV export/import (embeddings caching)
- Data validation (ensure 4 categories present)
- Train/test split handling

**Embedding Storage:**
- Store embeddings as numpy arrays (.npy format)
- Metadata storage (document_id, cluster_id, category_label)
- Efficient retrieval for clustering and classification

**Output Generation:**
- Export cluster assignments (CSV)
- Export classification results (JSON)
- Export performance metrics (JSON)
- Generate visualizations (PNG/PDF)

### Code Structure Requirements

**Recommended Project Structure:**
```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # AG News dataset
â”‚   â”œâ”€â”€ embeddings/             # Cached embeddings
â”‚   â””â”€â”€ clusters/               # Cluster assignments
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embedding_service.py   # Gemini API wrapper
â”‚   â”œâ”€â”€ clustering.py           # K-Means implementation
â”‚   â”œâ”€â”€ agent.py                # SpecializedAgent class
â”‚   â”œâ”€â”€ router.py               # AgentRouter class
â”‚   â”œâ”€â”€ baseline.py             # Baseline system
â”‚   â””â”€â”€ metrics.py              # Performance measurement
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb          # Experimental analysis
â”œâ”€â”€ visualizations/             # Generated plots
â”œâ”€â”€ results/                    # Metrics and outputs
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**Code Quality Standards:**
- PEP 8 compliance (use `black` or `autopep8` for formatting)
- Type hints for function signatures
- Docstrings for all classes and functions (Google style)
- Error handling with informative messages
- Logging for debugging and monitoring

### Technology Stack Specification

**Core Libraries:**
- `google-genai` (v0.3.0+) - Gemini API SDK
- `scikit-learn` (v1.7.2+) - K-Means clustering, metrics
- `numpy` (v1.24+) - Array operations
- `pandas` (v2.0+) - Data manipulation
- `datasets` (v2.14+) - Hugging Face datasets (AG News)

**Visualization:**
- `matplotlib` (v3.7+) - Basic plotting
- `seaborn` (v0.12+) - Statistical visualizations
- `plotly` (optional) - Interactive plots

**Development Tools:**
- `jupyter` - Interactive notebooks for analysis
- `pytest` - Unit testing (if time permits)
- `black` - Code formatting

### API Integration Requirements

**Google Gemini Embedding API:**
- **Authentication:** API key via environment variable `GEMINI_API_KEY`
- **Model:** `gemini-embedding-001`
- **Batch Processing:** Use Batch API for bulk embedding (cost efficiency)
- **Rate Limiting:** Respect API rate limits, implement exponential backoff
- **Error Handling:** Handle network errors, API errors gracefully

**API Call Patterns:**
```python
# Batch embedding (initialization)
embeddings = client.models.embed_content_batch(
    model="gemini-embedding-001",
    contents=document_list
)

# Single embedding (query classification)
query_embedding = client.models.embed_content(
    model="gemini-embedding-001",
    contents=query_text
)
```

### Configuration Management

**Configuration File (config.yaml):**
```yaml
# Dataset Configuration
dataset:
  name: "ag_news"
  categories: 4
  sample_size: null  # null = full dataset

# Clustering Configuration
clustering:
  algorithm: "kmeans"
  n_clusters: 4
  random_state: 42
  max_iter: 300
  init: "k-means++"

# Embedding Configuration
embedding:
  model: "gemini-embedding-001"
  batch_size: 100
  cache_dir: "data/embeddings"

# Classification Configuration
classification:
  method: "cosine_similarity"
  threshold: 0.7  # minimum similarity for confident classification

# Metrics Configuration
metrics:
  cost_per_1M_tokens_under_200k: 3.0   # USD
  cost_per_1M_tokens_over_200k: 6.0    # USD
  target_cost_reduction: 0.90          # 90%
```

### Reproducibility Requirements

**For Academic Submission:**
- Fixed random seeds (`random_state=42` for K-Means)
- Versioned dependencies (`requirements.txt` with exact versions)
- Clear instructions in README for environment setup
- Sample output included (pre-generated visualizations)
- Execution time estimates documented

---

## Functional Requirements

### FR-1: Dataset Loading and Preprocessing

**Description:** System must load and prepare the AG News dataset for embedding generation and clustering.

**Acceptance Criteria:**
- âœ… Load AG News dataset from Hugging Face `datasets` library
- âœ… Validate dataset contains 4 categories (World, Sports, Business, Sci/Tech)
- âœ… Extract text fields (title + description) for each document
- âœ… Handle train/test split (120K training, 7.6K test samples)
- âœ… Support optional sampling for faster experimentation
- âœ… Log dataset statistics (category distribution, sample sizes)

**Priority:** Critical (P0)

---

### FR-2: Embedding Generation

**Description:** Generate semantic embeddings for all documents using Google Gemini Embedding API.

**Acceptance Criteria:**
- âœ… Interface with Gemini Embedding API (`gemini-embedding-001`)
- âœ… Use Batch API for bulk embedding generation (cost optimization)
- âœ… Generate embeddings for all training documents
- âœ… Cache embeddings to disk (`.npy` format) to avoid redundant API calls
- âœ… Handle API errors with retry logic and exponential backoff
- âœ… Validate embedding dimensions (expected: 768)
- âœ… Track API usage and estimated costs

**Priority:** Critical (P0)

**Related Metrics:**
- Embedding generation time
- API call count
- Cache hit rate

---

### FR-3: K-Means Clustering

**Description:** Apply K-Means clustering algorithm to partition documents into K=4 semantic clusters.

**Acceptance Criteria:**
- âœ… Implement K-Means using scikit-learn
- âœ… Configure parameters:
  - K=4 clusters
  - `random_state=42` for reproducibility
  - `init='k-means++'` for better initialization
  - `max_iter=300`
- âœ… Fit clustering model on document embeddings
- âœ… Extract and store cluster centroids
- âœ… Assign each document to a cluster
- âœ… Export cluster assignments with metadata

**Priority:** Critical (P0)

**Validation:**
- All documents assigned to exactly one cluster
- Centroids have correct dimensionality (768)
- Cluster distribution is balanced (no extreme skew)

---

### FR-4: Cluster Quality Evaluation

**Description:** Evaluate the quality and separation of generated clusters using standard metrics.

**Acceptance Criteria:**
- âœ… Calculate Silhouette Score (target: >0.3)
- âœ… Calculate Davies-Bouldin Index (lower is better)
- âœ… Compute intra-cluster distance (compactness)
- âœ… Compute inter-cluster distance (separation)
- âœ… Generate evaluation report with all metrics
- âœ… Compare cluster assignments with ground truth labels (AG News categories)

**Priority:** High (P1)

**Success Threshold:**
- Silhouette Score >0.3 (indicates good separation)
- High alignment with AG News ground truth categories

---

### FR-5: Cluster Visualization

**Description:** Generate visual representations of clusters to demonstrate semantic separation.

**Acceptance Criteria:**
- âœ… Apply PCA dimensionality reduction (768D â†’ 2D)
- âœ… Generate scatter plot with 4 colored clusters
- âœ… Mark cluster centroids on visualization
- âœ… Add legend with cluster labels
- âœ… Export visualization as PNG/PDF (high resolution for report)
- âœ… Optional: Interactive plot using Plotly

**Priority:** Critical (P0) - Core "wow moment" visualization

**Visual Quality:**
- Clear color separation between clusters
- Minimal overlap between groups
- Professional formatting for academic report

---

### FR-6: Specialized Agent Implementation

**Description:** Implement agent classes that hold context subsets and process queries.

**Acceptance Criteria:**
- âœ… Define `SpecializedAgent` class with:
  - `cluster_id`: int
  - `context_documents`: list of documents from cluster
  - `process_query(query: str) -> str`: method to handle queries
- âœ… Create 4 agent instances, one per cluster
- âœ… Each agent maintains only documents from its assigned cluster (~25% of total)
- âœ… Implement agent registry (map cluster_id â†’ agent)
- âœ… Log agent initialization (cluster size, context length)

**Priority:** Critical (P0)

**Implementation Notes:**
- For MVP, agents can be simple wrappers (no actual LLM calls needed for demo)
- Focus on demonstrating context reduction (1/K of original size)

---

### FR-7: Query Classification and Routing

**Description:** Classify new queries to appropriate clusters using cosine similarity and route to corresponding agents.

**Acceptance Criteria:**
- âœ… Implement `AgentRouter` class with:
  - `classify_query(query_embedding) -> cluster_id`: classification logic
  - `route_query(query: str) -> SpecializedAgent`: routing logic
- âœ… Compute cosine similarity between query embedding and all cluster centroids
- âœ… Assign query to cluster with highest similarity score
- âœ… Return corresponding SpecializedAgent
- âœ… Track classification decisions for accuracy measurement
- âœ… Log similarity scores and routing decisions

**Priority:** Critical (P0)

**Performance Requirements:**
- Classification time <1 second per query
- Support batch classification for test set evaluation

---

### FR-8: Classification Accuracy Measurement

**Description:** Measure how accurately the system routes queries to correct semantic clusters.

**Acceptance Criteria:**
- âœ… Evaluate on AG News test set (7.6K samples with ground truth labels)
- âœ… Calculate overall classification accuracy (% correct cluster assignments)
- âœ… Generate confusion matrix (4x4 grid)
- âœ… Calculate per-cluster precision, recall, F1-score
- âœ… Identify misclassification patterns
- âœ… Export results as JSON and visualizations

**Priority:** Critical (P0)

**Success Threshold:**
- Overall accuracy >80%
- No cluster with accuracy <70%

---

### FR-9: Baseline System Implementation

**Description:** Implement naive baseline system for cost comparison (single agent with full context).

**Acceptance Criteria:**
- âœ… Implement `BaselineSystem` class:
  - Single agent with ALL documents (no clustering)
  - Simulates current LLM usage pattern
- âœ… Process same test queries as optimized system
- âœ… Track token consumption for each query
- âœ… Calculate estimated API costs
- âœ… Log performance metrics

**Priority:** Critical (P0) - Required for cost comparison

**Baseline Simulation:**
- Assume baseline sends full 200K+ token context per query
- Use actual token counts from AG News dataset

---

### FR-10: Cost Calculation and Comparison

**Description:** Calculate and compare LLM API costs between baseline and optimized systems.

**Acceptance Criteria:**
- âœ… Implement cost calculation using pricing tiers:
  - <200K tokens: $3.00 per 1M input tokens
  - â‰¥200K tokens: $6.00 per 1M input tokens (Claude Sonnet 4.5 pricing)
- âœ… Track for both systems:
  - Number of API calls
  - Total tokens per query
  - Cost per query
  - Total cost for test set
- âœ… Calculate cost reduction percentage
- âœ… Generate cost comparison visualization (bar chart)
- âœ… Export detailed cost breakdown (CSV/JSON)

**Priority:** Critical (P0)

**Success Threshold:**
- Demonstrate >90% cost reduction
- Clear visual comparison showing savings

---

### FR-11: Performance Metrics Tracking

**Description:** Track comprehensive performance metrics across all system components.

**Metrics to Track:**

**Clustering Metrics:**
- Silhouette Score
- Davies-Bouldin Index
- Cluster sizes (distribution)
- Convergence iterations

**Classification Metrics:**
- Overall accuracy
- Per-cluster precision, recall, F1
- Confusion matrix
- Classification time per query

**Cost Metrics:**
- Baseline: API calls, tokens, cost
- Optimized: API calls, tokens, cost
- Cost reduction percentage
- Cost per query (baseline vs optimized)

**System Metrics:**
- Embedding generation time
- Clustering execution time
- Query processing time

**Acceptance Criteria:**
- âœ… All metrics automatically calculated
- âœ… Export as structured JSON
- âœ… Generate summary report

**Priority:** High (P1)

---

### FR-12: Experimental Report Generation

**Description:** Generate comprehensive experimental report documenting methodology, results, and analysis.

**Report Sections:**

**1. Introduction**
- Problem statement (LLM cost optimization)
- Proposed approach (clustering + classification)
- Research objectives

**2. Methodology**
- Dataset description (AG News)
- K-Means clustering algorithm explanation
- Cosine similarity classification explanation
- Evaluation metrics

**3. Experimental Setup**
- Technology stack
- Configuration parameters
- Implementation details

**4. Results**
- Cluster visualization (PCA plot)
- Cluster quality metrics (Silhouette Score)
- Classification performance (accuracy, confusion matrix)
- Cost comparison (baseline vs optimized)
- All visualizations embedded

**5. Discussion**
- Analysis of results
- Connection to data mining course concepts
- Limitations and potential improvements

**6. Conclusion**
- Summary of findings
- Achieved cost reduction
- Validation of approach

**Acceptance Criteria:**
- âœ… All sections completed with clear writing
- âœ… All visualizations included (high quality)
- âœ… Quantitative results presented in tables
- âœ… Discussion connects to K-Means and classification theory
- âœ… Professional formatting (academic style)

**Priority:** Critical (P0) - Required deliverable

---

### FR-13: Code Documentation and Reproducibility

**Description:** Ensure code is well-documented and experiments are reproducible.

**Acceptance Criteria:**
- âœ… All functions have docstrings (Google style)
- âœ… Type hints for function signatures
- âœ… Inline comments for complex logic
- âœ… README.md with:
  - Project overview
  - Installation instructions
  - Usage examples
  - Experiment execution steps
  - Expected outputs
- âœ… requirements.txt with exact versions
- âœ… Configuration file (config.yaml) documented
- âœ… Sample output included in repository

**Priority:** High (P1)

**Reproducibility Checklist:**
- Fixed random seeds documented
- Environment setup instructions clear
- Expected runtime documented
- All dependencies specified

---

### FR-14: Visualization Suite

**Description:** Generate all required visualizations for experimental report and demo.

**Required Visualizations:**

1. **Cluster Visualization (PCA 2D)**
   - Scatter plot with 4 colored clusters
   - Cluster centroids marked
   - Legend with cluster labels
   - Professional formatting

2. **Cost Comparison Chart**
   - Bar chart: baseline cost vs optimized cost
   - Cost reduction percentage annotated
   - Clear labels and title

3. **Confusion Matrix**
   - 4x4 heatmap
   - True labels vs predicted labels
   - Color-coded for clarity

4. **Silhouette Analysis Plot**
   - Silhouette scores per cluster
   - Average score line
   - Cluster size distribution

5. **Classification Accuracy Plot** (optional)
   - Per-cluster accuracy bars
   - Overall accuracy line

**Acceptance Criteria:**
- âœ… All visualizations generated programmatically
- âœ… High resolution export (300 DPI minimum)
- âœ… Consistent color scheme and formatting
- âœ… Suitable for academic report inclusion

**Priority:** Critical (P0)

---

### Summary of Functional Requirements

**Critical (P0) - Must Have:**
- FR-1: Dataset Loading
- FR-2: Embedding Generation
- FR-3: K-Means Clustering
- FR-5: Cluster Visualization
- FR-6: Specialized Agents
- FR-7: Query Routing
- FR-8: Classification Accuracy
- FR-9: Baseline System
- FR-10: Cost Comparison
- FR-12: Experimental Report
- FR-14: Visualizations

**High Priority (P1) - Should Have:**
- FR-4: Cluster Quality Evaluation
- FR-11: Performance Metrics
- FR-13: Documentation

**Total:** 14 functional requirements organized by capability

---

## Non-Functional Requirements

### NFR-1: Performance

**Objective:** Ensure system performs efficiently within academic demonstration constraints.

**Requirements:**

**Query Classification Speed:**
- Query embedding generation + classification: <1 second per query
- Batch classification of test set (7.6K queries): <15 minutes total
- Cosine similarity computation: <10ms for K=4 clusters

**Clustering Performance:**
- K-Means convergence: <5 minutes for 120K documents
- PCA dimensionality reduction: <2 minutes for visualization
- Silhouette Score calculation: <3 minutes

**Embedding Generation:**
- Batch embedding via Gemini API: acceptable latency (network-dependent)
- Embedding caching: <1 second to load cached embeddings from disk
- No specific throughput requirements (one-time batch processing acceptable)

**Rationale:** This is an academic proof-of-concept, not a production system. Performance targets focus on reasonable experimentation times within 3-day timeline.

---

### NFR-2: Cost Efficiency

**Objective:** Minimize API costs while demonstrating cost optimization concept.

**Requirements:**

**Embedding API Costs:**
- Use Gemini Batch API ($0.075/1M tokens) for initial dataset embedding
- Cache embeddings aggressively to avoid redundant API calls
- Target: <$5 total for embedding generation (120K documents)

**LLM API Costs (Demonstration):**
- For cost comparison demonstration, use token counting (not actual LLM calls)
- Simulate costs based on context size and pricing tiers
- No actual LLM inference required for MVP (agents can be stubs)

**Overall Cost Target:**
- Total project API costs: <$10 (mostly embedding generation)
- Demonstrate >90% cost savings in comparison metrics

**Rationale:** Academic project with limited budget. Focus on demonstrating cost optimization methodology rather than running expensive LLM inference.

---

### NFR-3: Reliability and Error Handling

**Objective:** Handle errors gracefully and provide informative feedback.

**Requirements:**

**API Error Handling:**
- Gemini API failures: Implement exponential backoff retry (max 3 retries)
- Network timeouts: Clear error messages with retry suggestions
- Rate limiting: Detect and wait with appropriate backoff
- Authentication errors: Validate API key at startup with helpful error message

**Data Validation:**
- Validate dataset structure before processing
- Check embedding dimensions match expected (768)
- Verify cluster assignments are valid (no missing/duplicate assignments)
- Validate configuration file schema

**Graceful Degradation:**
- If embedding cache missing: regenerate (with warning)
- If visualization fails: continue with text-based metrics
- Log all errors with timestamps and context

**Rationale:** Academic demo must be robust enough for instructor evaluation. Clear error messages help with debugging during tight 3-day timeline.

---

### NFR-4: Reproducibility

**Objective:** Ensure experiments produce consistent, verifiable results.

**Requirements:**

**Deterministic Execution:**
- Fixed random seed for K-Means: `random_state=42`
- Fixed random seed for data sampling (if used): documented
- Numpy random seed set globally if needed
- No dependence on system-specific randomness

**Environment Consistency:**
- Exact dependency versions in `requirements.txt`
- Python version specified (3.9+ recommended)
- Operating system compatibility documented (tested on macOS/Linux/Windows)
- Instructions for virtual environment setup

**Result Verification:**
- Include expected output samples in repository
- Document expected metric ranges (e.g., Silhouette Score: 0.3-0.5)
- Provide checksum or sample embeddings for validation
- Execution logs saved automatically

**Rationale:** Course instructors must be able to reproduce results. Scientific reproducibility is a core academic requirement.

---

### NFR-5: Usability and Documentation

**Objective:** Make system easy to understand, run, and evaluate.

**Requirements:**

**Code Readability:**
- PEP 8 compliance (automated with `black` formatter)
- Descriptive variable names (no single-letter variables except loop counters)
- Maximum function length: 50 lines (encourage modular design)
- Type hints on all function signatures

**Documentation Quality:**
- README.md covers: installation, usage, troubleshooting
- Inline comments for non-obvious logic (especially clustering/classification)
- Docstrings for all public classes and methods (Google style)
- Configuration file has inline comments explaining each parameter

**Ease of Execution:**
- Single command to install dependencies: `pip install -r requirements.txt`
- Clear execution workflow documented
- Estimated runtime for each major step
- Progress logging to console (e.g., "Processing batch 1/10...")

**Visualization Quality:**
- High-resolution exports (300 DPI) for report inclusion
- Professional color schemes (colorblind-friendly)
- Clear axis labels, titles, legends
- Consistent formatting across all plots

**Rationale:** Academic evaluators need to quickly understand methodology and verify implementation. Clear documentation demonstrates professionalism and aids comprehension.

---

### NFR-6: Maintainability

**Objective:** Code should be modular, testable, and easy to extend.

**Requirements:**

**Code Organization:**
- Separation of concerns: distinct modules for embedding, clustering, agents, metrics
- Configuration externalized (config.yaml, not hardcoded)
- No magic numbers (use named constants)
- DRY principle: no copy-pasted code blocks

**Modularity:**
- Each class has single responsibility
- Functions have clear inputs/outputs with type hints
- Swappable components (e.g., easy to try different clustering algorithms)
- Minimal coupling between modules

**Extensibility:**
- Easy to change K value for clustering
- Easy to swap embedding models
- Easy to add new metrics
- Easy to try different datasets

**Testing (Optional for MVP):**
- If time permits: basic unit tests for core functions
- At minimum: manual testing checklist documented
- Validation scripts to check data integrity

**Rationale:** While this is a 3-day project, good structure makes debugging easier and demonstrates software engineering best practices.

---

### NFR-7: Compatibility

**Objective:** System runs on standard academic computing environments.

**Requirements:**

**Python Version:**
- Support Python 3.9+ (avoid bleeding-edge features)
- Tested on Python 3.10 or 3.11 (document tested version)

**Operating System:**
- Primary: macOS (development environment)
- Should work on: Linux, Windows (no OS-specific code)
- Path handling: use `pathlib` for cross-platform compatibility

**Hardware Requirements:**
- RAM: 8GB minimum (16GB recommended for full dataset)
- Disk space: ~2GB for dataset + embeddings + results
- CPU: Any modern processor (no GPU required)
- Internet: Required for Gemini API calls

**Dependency Compatibility:**
- Use stable, widely-adopted library versions
- Avoid beta/alpha packages
- Pin major versions (e.g., `scikit-learn>=1.7,<2.0`)

**Rationale:** Course instructors may use different platforms. System should work on standard academic computing setups without special hardware.

---

### NFR-8: Data Privacy and Security

**Objective:** Handle API keys securely and protect any sensitive data.

**Requirements:**

**API Key Management:**
- Never hardcode API keys in source code
- Use environment variables (`GEMINI_API_KEY`)
- Include `.env.example` file (template without actual key)
- `.gitignore` includes `.env` file

**Data Handling:**
- AG News is public dataset (no privacy concerns)
- No collection of user data
- Cached embeddings stored locally (not transmitted)
- No logging of API keys or sensitive information

**Security Best Practices:**
- Use HTTPS for all API calls (default in `google-genai` SDK)
- Validate inputs to prevent injection (not critical for academic demo)
- Keep dependencies updated to avoid known vulnerabilities

**Rationale:** Demonstrates professional security awareness. Prevents accidental exposure of API keys in Git repository.

---

### Summary of Non-Functional Requirements

**Performance (NFR-1):**
- Classification: <1s per query
- Clustering: <5 min
- Batch processing: <15 min

**Cost Efficiency (NFR-2):**
- Total API costs: <$10
- Demonstrate >90% savings

**Reliability (NFR-3):**
- Robust error handling
- Graceful degradation
- Informative error messages

**Reproducibility (NFR-4):**
- Fixed random seeds
- Versioned dependencies
- Documented environment

**Usability (NFR-5):**
- PEP 8 compliance
- Comprehensive documentation
- Clear visualizations

**Maintainability (NFR-6):**
- Modular design
- Single responsibility
- Extensible architecture

**Compatibility (NFR-7):**
- Python 3.9+
- Cross-platform
- Standard hardware

**Security (NFR-8):**
- Secure API key handling
- No hardcoded secrets
- Privacy-conscious

---

## Implementation Planning

### Epic Breakdown Required

Requirements must be decomposed into epics and bite-sized stories (200k context limit).

**Next Step:** Run `workflow create-epics-and-stories` to create the implementation breakdown.

### Implementation Timeline (3 Days)

**Day 1: Data Preparation & Clustering**
- Set up development environment
- Load AG News dataset
- Generate embeddings using Gemini Batch API
- Apply K-Means clustering (K=4)
- Evaluate cluster quality (Silhouette Score)
- Generate PCA visualization

**Day 2: Agent System & Classification**
- Implement SpecializedAgent and AgentRouter classes
- Build baseline system (single agent with full context)
- Test classification accuracy on test set
- Measure token consumption and costs
- Generate confusion matrix

**Day 3: Experimentation & Reporting**
- Run comparative experiments (baseline vs optimized)
- Collect all performance metrics
- Generate all visualizations (cluster plots, cost charts)
- Write experimental report
- Prepare demo presentation
- Code cleanup and documentation

---

## References

### Foundation Documents

- **Product Brief:** [docs/product-brief-report-2025-11-08.md](docs/product-brief-report-2025-11-08.md)
  - Core vision and problem statement
  - Target outcomes and success criteria
  - MVP scope definition

- **Technical Research:** [docs/research-technical-2025-11-08.md](docs/research-technical-2025-11-08.md)
  - Technology stack evaluation
  - Gemini Embedding API analysis
  - K-Means clustering research
  - AG News dataset selection rationale
  - Architecture Decision Records (ADRs)

### Key Research Sources

**Embedding Models:**
- Google Gemini Embedding API Documentation (2025)
- MTEB Benchmark Rankings (2025)

**Clustering & Classification:**
- scikit-learn Documentation v1.7.2
- K-Means Text Clustering Tutorials
- Cosine Similarity for Classification

**Cost Optimization Evidence:**
- arXiv 2025: LLM Cost Optimization Research (90-98% reduction potential)
- Helicone 2025: Token Compression + Routing Strategies
- Industry applications: Cast AI, QC-Opt frameworks

---

## Next Steps

### Immediate Actions

1. **Environment Setup**
   ```bash
   pip install google-genai scikit-learn numpy pandas datasets matplotlib seaborn
   ```

2. **API Configuration**
   - Obtain Gemini API key
   - Set environment variable: `export GEMINI_API_KEY="your-key"`

3. **Epic Breakdown** (Required)
   - Run: `workflow create-epics-and-stories`
   - Decompose 14 functional requirements into implementable stories

### Recommended Workflow Sequence

1. **Architecture Planning** (Recommended)
   - Run: `workflow create-architecture`
   - Document technical decisions and system design

2. **Implementation**
   - Follow 3-day timeline defined above
   - Track progress using story status

3. **Validation**
   - Run: `workflow validate-prd`
   - Ensure all requirements met before submission

---

## Summary

### What Makes This PRD Special

This PRD captures the essence of a **data mining course project that demonstrates clustering and classification algorithms through visual clarity and cost impact**.

**The Product Magic:**
When you see the PCA cluster visualization with 4 clearly separated semantic groups, you immediately understand WHY clustering works for LLM cost optimization. The visual separation proves that K-Means captures meaningful semantic boundariesâ€”and those boundaries drive the 90% cost savings.

**Key Differentiators:**
1. **Visual-First Approach** - Cluster visualization as the core "wow moment"
2. **Academic Rigor** - Clear demonstration of K-Means and cosine similarity techniques
3. **Real-World Impact** - Solves expensive LLM cost problem with fundamental algorithms
4. **Rapid Execution** - 3-day timeline with proven technology stack
5. **Reproducible Science** - Fixed seeds, versioned dependencies, clear methodology

### Requirements Summary

- **14 Functional Requirements** (11 Critical P0, 3 High Priority P1)
- **8 Non-Functional Requirements** (Performance, Cost, Reliability, Reproducibility, Usability, Maintainability, Compatibility, Security)
- **3-Tier Scope** (MVP, Growth, Vision)
- **Clear Success Metrics** (>90% cost reduction, >80% classification accuracy, Silhouette >0.3)

### Ready for Implementation

This PRD provides:
- âœ… Clear vision aligned with academic goals
- âœ… Detailed functional and non-functional requirements
- âœ… Specific acceptance criteria for each requirement
- âœ… Technology stack with rationale
- âœ… 3-day implementation timeline
- âœ… Success metrics and validation criteria

**Next:** Run epic breakdown workflow to create implementable stories.

---

_This PRD was created through collaborative discovery between Jack YUAN and AI Product Manager on 2025-11-08._

_The magic of this projectâ€”visualizing how clustering and classification solve real-world LLM cost problemsâ€”is woven throughout every requirement._
